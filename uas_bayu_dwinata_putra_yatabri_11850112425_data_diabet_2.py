# -*- coding: utf-8 -*-
"""UAS_Bayu Dwinata Putra Yatabri_11850112425 Data_diabet 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_8zp872BkQFe0n3TT3I55uqMt4RHCv-U
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import train_test_split
from scipy.stats import randint
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV
# %matplotlib inline

# reading csv files
#dataset =  pd.read_csv('house.data', sep=",")
#dataset.head

# reading csv files
feature_cols = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']
data = pd.read_csv('data_diabet 2.csv', names=feature_cols)
data.head()

data.info()

data.isnull().sum()

#from pandas_profiling import ProfileReport
#import pandas as pd

#prof = ProfileReport(data)
#prof
data.describe()

# Visualisasikan setiap kolom -> detection of outliers
plt.figure(figsize=(15, 10))
sns.boxplot(x="variable", y="value", data=pd.melt(data))
sns.stripplot(x="variable", y="value", data=pd.melt(data), color="orange", jitter=0.2, size=2.5)
plt.title("Outliers", loc="left")
plt.grid()

# Plot density of each column
import warnings
warnings.filterwarnings("ignore")

a=1
plt.figure(figsize=(20, 10))
for i in data.columns:
    plt.subplot(3, 3, a)
    sns.distplot(data[i])
    a += 1
plt.show()

# Outlier handling when features are normally or approximatelx normally distributed
def Z_Score_outlier_handling(column):
    upper_limit = data[column].mean() + 3*data[column].std()
    lower_limit = data[column].mean() - 3*data[column].std()
    data[column] = np.where(
        data[column] > upper_limit, upper_limit,
        np.where(data[column] < lower_limit, lower_limit, 
        data[column])
    )

# Outlier handling when features are skewed
def IQR_outlier_handling(column):
    percentile25 = data[column].quantile(0.25)
    percentile75 = data[column].quantile(0.75)
    iqr = percentile75-percentile25

    upper_limit = percentile75 + 1.5 * iqr
    lower_limit = percentile25 - 1.5 * iqr
    data[data[column] > upper_limit]
    data[data[column] < lower_limit]
    
    new_df = data[data[column] < upper_limit]
    new_df.shape
    
    data[column] = np.where(
    data[column] > upper_limit,
    upper_limit,
    np.where(
        data[column] < lower_limit,
        lower_limit,
        data[column]
        )
    )

# Outlier treatment by column
Z_Score_outlier_handling("Glucose")
Z_Score_outlier_handling("BloodPressure")
Z_Score_outlier_handling("SkinThickness")
Z_Score_outlier_handling("BMI")

IQR_outlier_handling("Pregnancies")
IQR_outlier_handling("Insulin")
IQR_outlier_handling("DiabetesPedigreeFunction")
IQR_outlier_handling("Age")

# # Plot density of each column after outlierhandling
a=1
plt.figure(figsize=(20, 10))
for i in data.columns:
    plt.subplot(3, 3, a)
    sns.distplot(data[i])
    a += 1
plt.show()

# Visualize correlations of each column (not necessary but for interest)
correlations = data.corr(method="pearson")
plt.figure(figsize=(10, 8))
sns.heatmap(correlations, vmin= -1, cmap="coolwarm", annot=True)

# Split X and Y and Train and Test Set
x = data.drop(columns=["Outcome"])
y = data["Outcome"]

# Show distribution of 0 and 1
y.value_counts()

# Fix distribution to make model more accurate (optional)
from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=42)
x, y = sm.fit_resample(x, y)
y.value_counts()

# Split train and test dataset
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=14)

# Commented out IPython magic to ensure Python compatibility.
from sklearn.svm import SVC, LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier, GradientBoostingClassifier, BaggingClassifier
from sklearn.metrics import classification_report
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import RandomizedSearchCV
# %matplotlib inline

# Create empty list and append each model to list
models = []
models.append(("SVC", SVC(random_state=14)))
models.append(("SVM", LinearSVC(random_state=14)))
models.append(("LOGR", LogisticRegression(solver="liblinear", random_state=14)))
models.append(("LDA", LinearDiscriminantAnalysis()))
models.append(("KNN", KNeighborsClassifier()))
models.append(("CART", DecisionTreeClassifier(random_state=14)))
models.append(("NB", GaussianNB()))
models.append(("DT", DecisionTreeClassifier(random_state=14)))
models.append(("RF", RandomForestClassifier(random_state=14)))
models.append(("ET", ExtraTreesClassifier(random_state=14)))
models.append(("GB", GradientBoostingClassifier(random_state=14)))
models.append(("BC", BaggingClassifier(random_state=14)))

# Empty lsit for results of the evaluation
model_results = []

# Function: for each element in model list there will be an evaluation -> Results will be added to results df
def train_all_models(models):
    i = 1
    plt.figure(figsize=(25, 15))
    for method, model in models:
        model.fit(x_train, y_train)
        test_pred = model.predict(x_test)

        f_score = model.score(x_test, y_test)
        model_results.append((method, f_score))

        plt.subplot(3, 4, i)
        plt.subplots_adjust(hspace=0.3, wspace=0.3)
        sns.heatmap(confusion_matrix(y_test, test_pred), annot=True, cmap="Greens")
        plt.title(model, fontsize=14)
        plt.xlabel('Test', fontsize=12)
        plt.ylabel('Predict', fontsize=12)
        df = pd.DataFrame(model_results).transpose()
        i+=1

# Show confusion matrix for each trained model 
    plt.show()
    df = pd.DataFrame(model_results)
    return df

# Sort results df for later visualizations    
best_models = train_all_models(models)
best_models = best_models.sort_values([1], ascending=False)

best_models

# Visualize results in a bar chart
y_pos = np.arange(len(best_models[0]))
plt.figure(figsize=(10, 6))
plt.bar(y_pos, best_models[1], color=(0.2, 0.4, 0.6, 0.6))
plt.xticks(y_pos, best_models[0])
plt.title('F-Score of all trained models')
plt.xlabel('Model Type')
plt.ylabel('F-Score')
plt.show()

best_model = ExtraTreesClassifier(max_depth=179, max_features=3, n_estimators=71, random_state=20)
best_model.fit(x_train, y_train)
pred = best_model.predict(x_test)

print(f"Classification report\n {classification_report(pred, y_test)}")
print(f"Score = {best_model.score(x_test,y_test)}")